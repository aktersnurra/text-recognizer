"""Factorized attention with convolutional relative positional encodings."""
from torch import nn


class FactorAttention(nn.Module):
    """Factorized attention with relative positional encodings."""
    def __init__(self, dim: int, num_heads: int) -> None:
        pass

